# CHIMERA CAUSAL TOOLING - COMPLETE INTEGRATION

This package adds three standalone tool suites that complement Chimera's existing core causal system.

## ARCHITECTURE

```
┌─────────────────────────────────────────────────────────┐
│                   CHIMERA CORE                          │
│  ✅ Already deployed:                                   │
│    - ReplayMode (deterministic replay)                  │
│    - ShadowExecutor (counterfactual execution)          │
│    - SignalAttributionLedger (PnL tracking)             │
│    - CounterfactualEngine (signal testing)              │
│    - ReplayController (orchestration)                   │
└─────────────────────────────────────────────────────────┘
                           ↓
           ┌───────────────┼───────────────┐
           ↓               ↓               ↓
┌──────────────────┐ ┌──────────────┐ ┌──────────────┐
│  CAUSAL LAB      │ │  DASHBOARD   │ │ ALPHA GOV    │
│  (Offline)       │ │  (Live)      │ │ (Auto-tune)  │
└──────────────────┘ └──────────────┘ └──────────────┘
```

## PHASE 7: CAUSAL LAB (Offline Analysis)

### Purpose
Batch processing and deep analysis of historical event logs.

### Tools
- **chimera_replay**: Replay event logs deterministically
- **chimera_attrib**: Compute signal-level attribution
- **chimera_batch**: Batch process multiple days

### Build
```bash
cd causal_lab
mkdir build && cd build
cmake ..
make -j
```

### Usage
```bash
# Replay single day
./chimera_replay ../data/events_20250121.bin

# Compute attribution
./chimera_attrib ../data/events_20250121.bin results.csv

# Batch process
./chimera_batch ../data/events/ monthly_report.csv
```

## PHASE 8: LIVE DASHBOARD

### Purpose
Real-time monitoring of causal attribution during live trading.

### Components
- FastAPI backend with WebSocket streaming
- React frontend with Tailwind CSS
- Live signal contribution bars

### Run
```bash
# Terminal 1: Start backend
cd causal_dashboard/server
./run.sh

# Terminal 2: Open browser
open http://localhost:8088/
```

### Data Feed
Dashboard reads from `research.csv` generated by chimera_attrib or core system.

## PHASE 9: ALPHA GOVERNOR

### Purpose
Automated signal quality monitoring and capital allocation.

### Features
- Continuous edge vs cost survival analysis
- Confidence-based signal throttling
- Auto-disable underperforming engines
- Dynamic capital reweighting

### Configuration
Edit `alpha_governor/config/governor.yaml`:
```yaml
min_edge_bps: 0.5      # Minimum edge to stay enabled
cost_bps: 0.8          # Execution cost threshold
min_confidence: 0.65   # Statistical confidence floor
```

### Run
```bash
cd alpha_governor/server
./run.sh
```

### Command Bridge
To wire governor commands into Chimera:
```bash
g++ -std=c++20 alpha_governor/server/command_bridge.cpp -o alpha_bridge
./alpha_bridge
```

## INTEGRATION WITH CORE CHIMERA

### Event Logging
Your core system should log events to binary format that causal_lab tools can read.

Example integration in main Chimera code:
```cpp
#include "chimera/audit/BinaryEventLog.hpp"

BinaryEventLog event_log("events/events_" + date_str + ".bin");

// Log fill
event_log.log(EventType::FILL, &fill_payload, sizeof(fill_payload));
```

### Research CSV Format
Both dashboard and alpha governor read from CSV with this schema:
```
trade_id,symbol,regime,ofi,impulse,spread,depth,toxic,vpin,funding,regime_contrib,total_pnl
```

Your core SignalAttributionLedger can export to this format.

## WORKFLOW

### Daily Batch Analysis
```bash
# 1. Collect event logs from live trading
# 2. Run offline attribution
./causal_lab/build/chimera_attrib events/today.bin research/today.csv

# 3. View results
cat research/today.csv | column -t -s,
```

### Live Monitoring
```bash
# 1. Start dashboard
cd causal_dashboard/server && ./run.sh &

# 2. Run live Chimera (logs to BinaryEventLog)
./chimera

# 3. Stream live attribution to research.csv
./causal_lab/build/chimera_attrib events/live.bin research.csv &

# 4. Monitor at http://localhost:8088
```

### Auto-Governance
```bash
# 1. Start alpha governor
cd alpha_governor/server && ./run.sh &

# 2. Governor watches research.csv
# 3. Emits commands to alpha_governor/logs/commands.out
# 4. Command bridge feeds back to Chimera control plane
./alpha_bridge
```

## FILES CREATED

```
causal_lab/
├── CMakeLists.txt
├── include/
│   ├── EventTypes.hpp
│   ├── EventBus.hpp
│   ├── ReplayEngine.hpp
│   ├── ShadowFarm.hpp
│   ├── AttributionEngine.hpp
│   └── RegimeStore.hpp
├── src/
│   ├── EventBus.cpp
│   ├── EventTypes.cpp
│   ├── ReplayEngine.cpp
│   ├── ShadowFarm.cpp
│   ├── AttributionEngine.cpp
│   └── RegimeStore.cpp
└── tools/
    ├── replay_main.cpp
    ├── attrib_main.cpp
    └── batch_main.cpp

causal_dashboard/
├── server/
│   ├── main.py
│   ├── requirements.txt
│   └── run.sh
└── web/
    └── public/
        ├── index.html
        └── main.jsx

alpha_governor/
├── config/
│   └── governor.yaml
├── server/
│   ├── main.py
│   ├── requirements.txt
│   ├── run.sh
│   └── command_bridge.cpp
└── logs/
    └── commands.out (created at runtime)
```

## TESTING

### Verify Causal Lab
```bash
cd causal_lab/build
./chimera_replay ../../test_data/sample.bin
```

### Verify Dashboard
```bash
cd causal_dashboard/server
python3 -c "import fastapi; print('OK')"
```

### Verify Alpha Governor
```bash
cd alpha_governor/server
python3 main.py &
curl http://localhost:8099/health
```

## TROUBLESHOOTING

**Problem**: Dashboard shows "No data"
**Solution**: Generate research.csv first using chimera_attrib

**Problem**: Alpha governor not issuing commands
**Solution**: Check research.csv has enough rows (min 200 for confidence)

**Problem**: Build errors in causal_lab
**Solution**: Ensure C++20 compiler (gcc 10+ or clang 11+)

## NEXT STEPS

1. Wire BinaryEventLog into your main Chimera loop
2. Export SignalAttributionLedger to CSV format
3. Run dashboard in background during live trading
4. Connect alpha_bridge to your CapitalAllocator
5. Monitor signal survival metrics daily

## SUPPORT

These tools are designed to be STANDALONE and NOT require changes to your existing core Chimera code. They consume data your system already produces (event logs, attribution CSVs) and provide additional analysis and monitoring capabilities.
